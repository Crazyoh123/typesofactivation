{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yOAbFjQ-j0Zi"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sigmoid Activation function -> The sigmoid function, also known as the logistic function, is a commonly used activation function in artificial neural networks. It maps any real-valued number to a value between 0 and 1, which makes it suitable for tasks involving binary classification or when you need to produce probabilities. EX: Binary Classification,Logistic Regression,Artificial Neural Networks,Recurrent Neural Networks (RNNs),Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTM) Networks,Logistic-based Models in Statistics\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "for i in range(0,75):\n",
        "     print(\"Iteartion size \"+str(i))\n",
        "     print(sigmoid(i * 0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj4SzCBUj7SX",
        "outputId": "567889ce-bd10-4574-ae47-d06454cc24b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteartion size 0\n",
            "0.5\n",
            "Iteartion size 1\n",
            "0.6224593312018546\n",
            "Iteartion size 2\n",
            "0.7310585786300049\n",
            "Iteartion size 3\n",
            "0.8175744761936437\n",
            "Iteartion size 4\n",
            "0.8807970779778823\n",
            "Iteartion size 5\n",
            "0.9241418199787566\n",
            "Iteartion size 6\n",
            "0.9525741268224334\n",
            "Iteartion size 7\n",
            "0.9706877692486436\n",
            "Iteartion size 8\n",
            "0.9820137900379085\n",
            "Iteartion size 9\n",
            "0.9890130573694068\n",
            "Iteartion size 10\n",
            "0.9933071490757153\n",
            "Iteartion size 11\n",
            "0.995929862284104\n",
            "Iteartion size 12\n",
            "0.9975273768433653\n",
            "Iteartion size 13\n",
            "0.998498817743263\n",
            "Iteartion size 14\n",
            "0.9990889488055994\n",
            "Iteartion size 15\n",
            "0.9994472213630764\n",
            "Iteartion size 16\n",
            "0.9996646498695336\n",
            "Iteartion size 17\n",
            "0.9997965730219448\n",
            "Iteartion size 18\n",
            "0.9998766054240137\n",
            "Iteartion size 19\n",
            "0.9999251537724895\n",
            "Iteartion size 20\n",
            "0.9999546021312976\n",
            "Iteartion size 21\n",
            "0.9999724643088853\n",
            "Iteartion size 22\n",
            "0.999983298578152\n",
            "Iteartion size 23\n",
            "0.9999898700090192\n",
            "Iteartion size 24\n",
            "0.9999938558253978\n",
            "Iteartion size 25\n",
            "0.9999962733607158\n",
            "Iteartion size 26\n",
            "0.999997739675702\n",
            "Iteartion size 27\n",
            "0.999998629042793\n",
            "Iteartion size 28\n",
            "0.9999991684719722\n",
            "Iteartion size 29\n",
            "0.9999994956525918\n",
            "Iteartion size 30\n",
            "0.999999694097773\n",
            "Iteartion size 31\n",
            "0.9999998144608981\n",
            "Iteartion size 32\n",
            "0.9999998874648379\n",
            "Iteartion size 33\n",
            "0.999999931743971\n",
            "Iteartion size 34\n",
            "0.9999999586006244\n",
            "Iteartion size 35\n",
            "0.999999974890009\n",
            "Iteartion size 36\n",
            "0.9999999847700205\n",
            "Iteartion size 37\n",
            "0.9999999907625504\n",
            "Iteartion size 38\n",
            "0.9999999943972036\n",
            "Iteartion size 39\n",
            "0.9999999966017321\n",
            "Iteartion size 40\n",
            "0.9999999979388463\n",
            "Iteartion size 41\n",
            "0.9999999987498471\n",
            "Iteartion size 42\n",
            "0.9999999992417439\n",
            "Iteartion size 43\n",
            "0.9999999995400946\n",
            "Iteartion size 44\n",
            "0.9999999997210531\n",
            "Iteartion size 45\n",
            "0.9999999998308102\n",
            "Iteartion size 46\n",
            "0.9999999998973812\n",
            "Iteartion size 47\n",
            "0.9999999999377585\n",
            "Iteartion size 48\n",
            "0.9999999999622486\n",
            "Iteartion size 49\n",
            "0.9999999999771028\n",
            "Iteartion size 50\n",
            "0.999999999986112\n",
            "Iteartion size 51\n",
            "0.9999999999915765\n",
            "Iteartion size 52\n",
            "0.999999999994891\n",
            "Iteartion size 53\n",
            "0.9999999999969011\n",
            "Iteartion size 54\n",
            "0.9999999999981204\n",
            "Iteartion size 55\n",
            "0.99999999999886\n",
            "Iteartion size 56\n",
            "0.9999999999993086\n",
            "Iteartion size 57\n",
            "0.9999999999995806\n",
            "Iteartion size 58\n",
            "0.9999999999997455\n",
            "Iteartion size 59\n",
            "0.9999999999998457\n",
            "Iteartion size 60\n",
            "0.9999999999999065\n",
            "Iteartion size 61\n",
            "0.9999999999999432\n",
            "Iteartion size 62\n",
            "0.9999999999999656\n",
            "Iteartion size 63\n",
            "0.9999999999999791\n",
            "Iteartion size 64\n",
            "0.9999999999999873\n",
            "Iteartion size 65\n",
            "0.9999999999999922\n",
            "Iteartion size 66\n",
            "0.9999999999999953\n",
            "Iteartion size 67\n",
            "0.9999999999999971\n",
            "Iteartion size 68\n",
            "0.9999999999999982\n",
            "Iteartion size 69\n",
            "0.9999999999999989\n",
            "Iteartion size 70\n",
            "0.9999999999999993\n",
            "Iteartion size 71\n",
            "0.9999999999999996\n",
            "Iteartion size 72\n",
            "0.9999999999999998\n",
            "Iteartion size 73\n",
            "0.9999999999999998\n",
            "Iteartion size 74\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The tanh() function stands for the hyperbolic tangent function. It is a popular activation function used in various artificial neural network architectures. The hyperbolic tangent function is a hyperbolic analog of the sigmoid function, and it maps input values to a range between -1 and 1. used in Hidden Layers in Neural Networks,Rescaling Data,Recurrent Neural Networks (RNNs)\n",
        "def tanh(x):\n",
        "  e = 2.71828\n",
        "  return (e**x - e**(-x)) / (e**x + e**(-x))\n",
        "\n",
        "for i in range(0,40):\n",
        "     print(\"Iteartion size \"+str(i))\n",
        "     print(tanh(i * 0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPXV6WgRlku3",
        "outputId": "20cb090d-8aec-4f41-ce19-2b87566fb768"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteartion size 0\n",
            "0.0\n",
            "Iteartion size 1\n",
            "0.4621168927568661\n",
            "Iteartion size 2\n",
            "0.7615938734587377\n",
            "Iteartion size 3\n",
            "0.9051480713154821\n",
            "Iteartion size 4\n",
            "0.9640274850287548\n",
            "Iteartion size 5\n",
            "0.9866142534330222\n",
            "Iteartion size 6\n",
            "0.9950547337774402\n",
            "Iteartion size 7\n",
            "0.9981778890394999\n",
            "Iteartion size 8\n",
            "0.9993292961310808\n",
            "Iteartion size 9\n",
            "0.9997532093541767\n",
            "Iteartion size 10\n",
            "0.999909203651881\n",
            "Iteartion size 11\n",
            "0.9999665969091533\n",
            "Iteartion size 12\n",
            "0.9999877115516064\n",
            "Iteartion size 13\n",
            "0.9999954793118734\n",
            "Iteartion size 14\n",
            "0.9999983369282833\n",
            "Iteartion size 15\n",
            "0.9999993881893732\n",
            "Iteartion size 16\n",
            "0.9999997749272536\n",
            "Iteartion size 17\n",
            "0.9999999172003021\n",
            "Iteartion size 18\n",
            "0.9999999695396722\n",
            "Iteartion size 19\n",
            "0.9999999887942639\n",
            "Iteartion size 20\n",
            "0.9999999958776372\n",
            "Iteartion size 21\n",
            "0.9999999984834663\n",
            "Iteartion size 22\n",
            "0.9999999994420982\n",
            "Iteartion size 23\n",
            "0.9999999997947593\n",
            "Iteartion size 24\n",
            "0.9999999999244962\n",
            "Iteartion size 25\n",
            "0.9999999999722237\n",
            "Iteartion size 26\n",
            "0.9999999999897815\n",
            "Iteartion size 27\n",
            "0.9999999999962408\n",
            "Iteartion size 28\n",
            "0.9999999999986172\n",
            "Iteartion size 29\n",
            "0.9999999999994913\n",
            "Iteartion size 30\n",
            "0.9999999999998128\n",
            "Iteartion size 31\n",
            "0.9999999999999313\n",
            "Iteartion size 32\n",
            "0.9999999999999748\n",
            "Iteartion size 33\n",
            "0.9999999999999906\n",
            "Iteartion size 34\n",
            "0.9999999999999966\n",
            "Iteartion size 35\n",
            "0.9999999999999989\n",
            "Iteartion size 36\n",
            "0.9999999999999996\n",
            "Iteartion size 37\n",
            "0.9999999999999998\n",
            "Iteartion size 38\n",
            "1.0\n",
            "Iteartion size 39\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ReLU stands for Rectified Linear Unit. It is a popular activation function used in artificial neural networks, especially in the hidden layers of deep learning architectures. The ReLU function is known for its simplicity, computational efficiency, and its ability to mitigate the vanishing gradient problem, which was a limitation of the sigmoid and tanh activation functions. where it is used Hidden Layers in Neural Networks,Efficient Computation,Vanishing Gradient Mitigation,Deep Learning Architectures.\n",
        "def relu(x):\n",
        "    return max(0, x)\n",
        "\n",
        "for i in range(0,3):\n",
        "     print(\"Iteartion size \"+str(i))\n",
        "     print(relu(i * 0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3iMOZuImBCa",
        "outputId": "130d959a-217c-42f5-b302-7297b2580391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteartion size 0\n",
            "0\n",
            "Iteartion size 1\n",
            "0.5\n",
            "Iteartion size 2\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Leaky ReLU (Rectified Linear Unit) is a variation of the ReLU activation function. It was introduced to address the \"dying ReLU\" problem, which occurs when ReLU neurons become inactive (output 0) for negative input values during training. The Leaky ReLU function allows a small, non-zero gradient for negative input values, which prevents neurons from becoming completely inactive and helps overcome the \"dying ReLU\" problem. where it is used Avoiding \"Dying ReLU\" Problem,Deep Learning Architectures,Variants.\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return max(alpha * x, x)\n",
        "\n",
        "for i in range(0,3):\n",
        "     print(\"Iteartion size \"+str(i))\n",
        "     print(leaky_relu(i * 0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL-XW6a9mVEP",
        "outputId": "16b8d922-a11d-40e1-d805-37d0ee2c40d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteartion size 0\n",
            "0.0\n",
            "Iteartion size 1\n",
            "0.5\n",
            "Iteartion size 2\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PreLU (Parametric Rectified Linear Unit) is another variation of the ReLU activation function. It is similar to Leaky ReLU but with the difference that the parameter controlling the slope for negative input values is learned during the training process rather than being a fixed constant. Used in Convolutional Neural Networks (CNNs): PReLU has been widely used as an activation function in CNNs for image classification, object detection, segmentation, and other computer vision tasks. Its ability to learn negative slopes helps prevent the dying ReLU problem, where neurons can become inactive during training.used in Image Recognition and Classification:Natural Language Processing (NLP),Speech Recognition:,Generative Adversarial Networks,Reinforcement Learning.\n",
        "def prelu(x, alpha):\n",
        "    return max(alpha * x, x)\n",
        "\n",
        "for i in range(0, 3):\n",
        "    print(\"Iteration size \" + str(i))\n",
        "    print(prelu(i, 0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWkPoeWQmhel",
        "outputId": "2bc4e220-6145-4961-9718-5006dc8ae8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration size 0\n",
            "0.0\n",
            "Iteration size 1\n",
            "1\n",
            "Iteration size 2\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The Exponential Linear Unit (ELU) activation function is another type of activation function used in deep learning models. It is designed to address some of the limitations of the traditional Rectified Linear Unit (ReLU) activation function, such as the dying ReLU problem. Used in Feedforward Neural Networks,Convolutional Neural Networks (CNNs),Transfer Learning,Reinforcement Learning.\n",
        "def elu(x, alpha=1.0):\n",
        "    return x if x >= 0 else alpha * (np.exp(x) - 1)\n",
        "\n",
        "for i in range(0,20):\n",
        "    print(\"Number of iteration :\"+str(i/2))\n",
        "    print(elu(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FVQv_HbnKXr",
        "outputId": "3d6f03c0-baf5-45eb-fa49-f60a2356e980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration :0.0\n",
            "0\n",
            "Number of iteration :0.5\n",
            "1\n",
            "Number of iteration :1.0\n",
            "2\n",
            "Number of iteration :1.5\n",
            "3\n",
            "Number of iteration :2.0\n",
            "4\n",
            "Number of iteration :2.5\n",
            "5\n",
            "Number of iteration :3.0\n",
            "6\n",
            "Number of iteration :3.5\n",
            "7\n",
            "Number of iteration :4.0\n",
            "8\n",
            "Number of iteration :4.5\n",
            "9\n",
            "Number of iteration :5.0\n",
            "10\n",
            "Number of iteration :5.5\n",
            "11\n",
            "Number of iteration :6.0\n",
            "12\n",
            "Number of iteration :6.5\n",
            "13\n",
            "Number of iteration :7.0\n",
            "14\n",
            "Number of iteration :7.5\n",
            "15\n",
            "Number of iteration :8.0\n",
            "16\n",
            "Number of iteration :8.5\n",
            "17\n",
            "Number of iteration :9.0\n",
            "18\n",
            "Number of iteration :9.5\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SELU stands for Scaled Exponential Linear Unit. It is an advanced activation function introduced to address the vanishing/exploding gradient problem and promote self-normalization in deep neural networks. The SELU activation function is designed to work specifically with certain types of neural network architectures, such as densely connected networks. used in Densely Connected Neural Networks,Autoencoders,Sequence-to-Sequence Models.\n",
        "def selu(x, alpha=1.6733, scale=1.0507):\n",
        "    return scale * elu(x, alpha)\n",
        "\n",
        "for i in range(0,20):\n",
        "    print(\"Number of iteration :\"+str(i/2))\n",
        "    print(selu(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa8qkrjin6Rw",
        "outputId": "1e2c9950-94c2-4745-87f0-0450565a4bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration :0.0\n",
            "0.0\n",
            "Number of iteration :0.5\n",
            "1.0507\n",
            "Number of iteration :1.0\n",
            "2.1014\n",
            "Number of iteration :1.5\n",
            "3.1521\n",
            "Number of iteration :2.0\n",
            "4.2028\n",
            "Number of iteration :2.5\n",
            "5.2535\n",
            "Number of iteration :3.0\n",
            "6.3042\n",
            "Number of iteration :3.5\n",
            "7.3549\n",
            "Number of iteration :4.0\n",
            "8.4056\n",
            "Number of iteration :4.5\n",
            "9.456299999999999\n",
            "Number of iteration :5.0\n",
            "10.507\n",
            "Number of iteration :5.5\n",
            "11.5577\n",
            "Number of iteration :6.0\n",
            "12.6084\n",
            "Number of iteration :6.5\n",
            "13.659099999999999\n",
            "Number of iteration :7.0\n",
            "14.7098\n",
            "Number of iteration :7.5\n",
            "15.7605\n",
            "Number of iteration :8.0\n",
            "16.8112\n",
            "Number of iteration :8.5\n",
            "17.8619\n",
            "Number of iteration :9.0\n",
            "18.912599999999998\n",
            "Number of iteration :9.5\n",
            "19.9633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Softmax is a mathematical function often used in machine learning and deep learning, specifically in multiclass classification problems. It is used to convert a vector of real numbers into a probability distribution, where the elements of the vector are transformed into values between 0 and 1, and the sum of the elements becomes 1. used in Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), where multiclass classification is required.\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "\n",
        "for i in range(0,20):\n",
        "    print(\"Number of iteration :\"+str(i/2))\n",
        "    print(softmax(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxZURSkjoCQx",
        "outputId": "9ab40270-61ab-49e8-b061-28252fba797f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of iteration :0.0\n",
            "1.0\n",
            "Number of iteration :0.5\n",
            "1.0\n",
            "Number of iteration :1.0\n",
            "1.0\n",
            "Number of iteration :1.5\n",
            "1.0\n",
            "Number of iteration :2.0\n",
            "1.0\n",
            "Number of iteration :2.5\n",
            "1.0\n",
            "Number of iteration :3.0\n",
            "1.0\n",
            "Number of iteration :3.5\n",
            "1.0\n",
            "Number of iteration :4.0\n",
            "1.0\n",
            "Number of iteration :4.5\n",
            "1.0\n",
            "Number of iteration :5.0\n",
            "1.0\n",
            "Number of iteration :5.5\n",
            "1.0\n",
            "Number of iteration :6.0\n",
            "1.0\n",
            "Number of iteration :6.5\n",
            "1.0\n",
            "Number of iteration :7.0\n",
            "1.0\n",
            "Number of iteration :7.5\n",
            "1.0\n",
            "Number of iteration :8.0\n",
            "1.0\n",
            "Number of iteration :8.5\n",
            "1.0\n",
            "Number of iteration :9.0\n",
            "1.0\n",
            "Number of iteration :9.5\n",
            "1.0\n"
          ]
        }
      ]
    }
  ]
}